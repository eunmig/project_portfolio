# project_portfolio


### PJT_01
------------------------
- 목표 : API 이해하기
- 서버는 어떻게 요청을 해석할까?

    URL, request.get(url), .json 형태 등 클라이언트들은 각자 다른 방식으로 요청을 보냄.
- API
    - 클라이언트가 원하는 기능을 수행하기 위해서 서버 측에 만들어 놓은 프로그램
- Open API

    외부에서 사용할 수 있도록 무료로 개방된 API

    오픈 API는 API KEY를 활용하여 사용자를 확인한다.
- 과제
    1. 외부 서버를 활용한 데이터 수집
    2. 요구사항에 맞게 JSON 형태 데이터 가공


### PJT_02
------------------------
- 목표 : 데이터 사이언스 이해하기
- 'Kaggle' 사이트 활용 _ 데이터 분석 경진대회 플랫폼
- Jupyter notebook

    - 데이터 사이언스 작업에 많이 활용되는 파이썬 개발 환경
    - 코드 실행, 텍스트 문서 작성, 시각화 등을 하나의 문서에 통합하여 작업 가능
- 데이터 사이언스
    - 다양한 데이터로부터 새로운 지식과 정보를 추출하기 위해 과학적 방법론, 프로세스, 알고리즘, 시스템을 동원하는 융합 분야
    - 컴퓨터 과학, 통계학, 수학 등 다양한 학문의 원리와 기술을 활용
    1. 문제 정의
    2. 데이터 수집
    3. 데이터 전처리(정제) : 실질적인 분석을 수행하기 위핻 데이터 가공
    4. 데이터 분석
    5. 결과 해석 및 공유 : 결과를 해석하고 시각화 후 공유하는 단계
- Numpy
    - 다차원 배열을 쉽게 처리하고 효율적으로 사용할 수 있도록 지원하는 파이썬 패키지
- Pandas
    - Numpy 기반으로 만들어진 패키지로, Series와 DataFrame 이라는 효율적인 자료구조 제공
- Matplotlib
    - Python 에서 데이터 시각화를 위해 가장 널리 사용되는 라이브러리
    - 다양한 종류의 그래프와 도표를 생성하고 데이터를 시각적으로 표현 할 수 있음
- 과제
    1. 넷플릭스 주가 데이터 분석


### PJT_04
------------------------
- 목표 : Django 에서 데이터 사이언스 패키지 사용하기
- 데이터 사이언스 패키지를 사용하기 위해 알아야 할 내용
    - 데이터 사이언스 3종 패키지 사용방법
    - django 기본 사용방법
    - 파이썬 ByteslO 패키지
- 과제
    1. 데이터 사이언스 패키지를 Django 에서 활용하는 방법 익히기


### PJT_05
------------------------
- 목표 : 웹 크롤링 이해하기
    - django 없이, 크롤링 하는 방법 학습
    - 구글 검색 수를 크롤링하여 어떤 키워드가 더 많이 검색되는 지 조사하기
- 웹 크롤링

    여러 웹 페이지를 돌아다니며 원하는 정보를 모으는 기술
    웹 사이트들을 돌아다니며 필요한 데이터를 추출하여 활용할 수 있도록 자동화된 프로세스
    1. 웹 페이지 다운로드
    2. 페이지 파싱
    3. 링크 추출 및 다른 페이지 탐색
    4. 데이터 추출 및 저장
- 과제
    1. 크롤링을 통한 데이터 수집
    2. 수집한 데이터를 DB에 저장하고, 저장한 데이터 활용하기


### PJT_06
------------------------
- 목표 : 관계형 DB 설계
- 과제
    1. 회원 관련 기능이 추가된 게시판 구현


### PJT_07
------------------------
- 목표 : API Server 제작
- 백엔드 개발 vs 프론트엔드 개발
    - 백엔드 : REST API 서버 개발
    - 프론트엔드 : REST API를 사용하여, 결과를 받아 화면 구성
-과제
    1. 정기예금 데이터를 활용한 REST API Server 구축하기
    2. 금융감독원 API를 활용한 데이터 수집